{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Type: Search Engine Building\n",
    "\n",
    "#### Group : Group 4\n",
    "\n",
    "#### Members : \n",
    "\n",
    "    Nupur Roy Chowdhury, nr572@drexel.edu\n",
    "    \n",
    "    Mahesh Sercat Ramakumar, ms4976@drexel.edu\n",
    "    \n",
    "    Rohith Lakshminarayana, rl669@drexel.edu\n",
    "    \n",
    "    Manisha Uttam Nandawadekar, mun24@drexel.edu\n",
    "    \n",
    "\n",
    "### Introduction:\n",
    "\n",
    "As part of our project we are using the News API: http://newsapi.org/  to collect news data for our project.\n",
    "\n",
    "In this notebook we are collecting, loading the data and indexing it into the elasstic search servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "import json, datetime, warnings\n",
    "from elasticsearch import Elasticsearch, RequestsHttpConnection\n",
    "from newsapi import NewsApiClient\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the server connections:\n",
    "\n",
    "In the below, we are establishing the connection to the Elasticsearch servers hosted in Drexel Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Elasticsearch_connection(host_link,user_auth):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    es = Elasticsearch(hosts=host_link ,verify_certs=False,http_auth= user_auth, connection_class=RequestsHttpConnection,)\n",
    "    print(\"ElasticSearch connection has been established and this connection instance is stored in es variable\")\n",
    "    return es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch_connection(['https://tux-es1.cci.drexel.edu:9200/','https://tux-es2.cci.drexel.edu:9200/',\n",
    "                               'https://tux-es3.cci.drexel.edu:9200/'],'ms4976:Phooh3ahkei7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the connection has got established, we are storing it in a variable called es. This variable will be in use below to create the index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are storing the key value in a global variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apikey  =  '50c30305b54a4857bef3a755586cc26e'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an index with default settings and  mappings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True,\n",
       " 'shards_acknowledged': True,\n",
       " 'index': 'ms4976_info624_201904_newsproject'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "index_name = 'ms4976_info624_201904_newsproject'\n",
    "\n",
    "request_body = {\n",
    "        'mappings': {\n",
    "            \n",
    "            \"properties\":{\n",
    "                \"source\":{\n",
    "                    \"type\": \"text\",\n",
    "                    \"analyzer\": \"standard\"\n",
    "                    },\n",
    "                \"author\":{\n",
    "                    \"type\": \"text\" ,\n",
    "                    \"analyzer\": \"standard\",\n",
    "                    \"similarity\": \"boolean\"\n",
    "                    },\n",
    "                \"title\":{\n",
    "                    \"type\": \"text\" ,\n",
    "                    \"analyzer\": \"english\",\n",
    "                    },\n",
    "     \n",
    "                \"description\":{\n",
    "                    \"type\": \"text\" ,\n",
    "                    \"analyzer\": \"english\",\n",
    "                    },\n",
    "                 \"url\":{\n",
    "                    \"type\": \"text\"\n",
    "                    },\n",
    "     \n",
    "                \"publishedAt\":{\n",
    "                    \"type\" : \"date\"\n",
    "                    },\n",
    "                \"timestamp\" :{\n",
    "                    \"type\" : \"rank_feature\",\n",
    "                    \"positive_score_impact\" : True  \n",
    "                    }\n",
    "                \n",
    "                }\n",
    "            }\n",
    "        }\n",
    "es.indices.create(index = index_name, body = request_body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Our main index for the project is: ms4976_info624_201904_newsproject\n",
    "\n",
    "* We have created this index with default mappings where:\n",
    "\n",
    "        We have used the analyzer: standard for the fields - source and author.\n",
    "    \n",
    "        We have used the analyzer: English for title and description.\n",
    "    \n",
    "        For the author field, we have used the boolean similarity.\n",
    "    \n",
    "        To rank the documents in a given order, we have used the rank_feature for the filed timestamp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting sources names:\n",
    "\n",
    "Below we have collected the names of all the 128 news channels/sources from where we would be collecting our data.\n",
    "\n",
    "The reason to do this is if we use only one name in the query field of the 'API' then, it returns only 20 values of the data.\n",
    "\n",
    "Hence, to collect more data, we have used multiple sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abc-news', 'abc-news-au', 'aftenposten', 'al-jazeera-english', 'ansa', 'argaam', 'ars-technica', 'ary-news', 'associated-press', 'australian-financial-review', 'axios', 'bbc-news', 'bbc-sport', 'bild', 'blasting-news-br', 'bleacher-report', 'bloomberg', 'breitbart-news', 'business-insider', 'business-insider-uk', 'buzzfeed', 'cbc-news', 'cbs-news', 'cnn', 'cnn-es', 'crypto-coins-news', 'der-tagesspiegel', 'die-zeit', 'el-mundo', 'engadget', 'entertainment-weekly', 'espn', 'espn-cric-info', 'financial-post', 'focus', 'football-italia', 'fortune', 'four-four-two', 'fox-news', 'fox-sports', 'globo', 'google-news', 'google-news-ar', 'google-news-au', 'google-news-br', 'google-news-ca', 'google-news-fr', 'google-news-in', 'google-news-is', 'google-news-it', 'google-news-ru', 'google-news-sa', 'google-news-uk', 'goteborgs-posten', 'gruenderszene', 'hacker-news', 'handelsblatt', 'ign', 'il-sole-24-ore', 'independent', 'infobae', 'info-money', 'la-gaceta', 'la-nacion', 'la-repubblica', 'le-monde', 'lenta', 'lequipe', 'les-echos', 'liberation', 'marca', 'mashable', 'medical-news-today', 'msnbc', 'mtv-news', 'mtv-news-uk', 'national-geographic', 'national-review', 'nbc-news', 'news24', 'new-scientist', 'news-com-au', 'newsweek', 'new-york-magazine', 'next-big-future', 'nfl-news', 'nhl-news', 'nrk', 'politico', 'polygon', 'rbc', 'recode', 'reddit-r-all', 'reuters', 'rt', 'rte', 'rtl-nieuws', 'sabq', 'spiegel-online', 'svenska-dagbladet', 't3n', 'talksport', 'techcrunch', 'techcrunch-cn', 'techradar', 'the-american-conservative', 'the-globe-and-mail', 'the-hill', 'the-hindu', 'the-huffington-post', 'the-irish-times', 'the-jerusalem-post', 'the-lad-bible', 'the-next-web', 'the-sport-bible', 'the-times-of-india', 'the-verge', 'the-wall-street-journal', 'the-washington-post', 'the-washington-times', 'time', 'usa-today', 'vice-news', 'wired', 'wired-de', 'wirtschafts-woche', 'xinhua-net', 'ynet']\n"
     ]
    }
   ],
   "source": [
    "newsapi = NewsApiClient(api_key=apikey)\n",
    "sources = newsapi.get_sources()\n",
    "news_terms = list()\n",
    "for news_name in sources[\"sources\"]:\n",
    "    news_terms.append(news_name['id']) \n",
    "print(news_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some popular news items category from kaggle news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CRIME', 'ENTERTAINMENT', 'WORLD NEWS', 'IMPACT', 'POLITICS', 'WEIRD NEWS', 'BLACK VOICES', 'WOMEN', 'COMEDY', 'QUEER VOICES', 'SPORTS', 'BUSINESS', 'TRAVEL', 'MEDIA', 'TECH', 'RELIGION', 'SCIENCE', 'LATINO VOICES', 'EDUCATION', 'COLLEGE', 'PARENTS', 'ARTS & CULTURE', 'STYLE', 'GREEN', 'TASTE', 'HEALTHY LIVING', 'THE WORLDPOST', 'GOOD NEWS', 'WORLDPOST', 'FIFTY', 'ARTS', 'WELLNESS', 'PARENTING', 'HOME & LIVING', 'STYLE & BEAUTY', 'DIVORCE', 'WEDDINGS', 'FOOD & DRINK', 'MONEY', 'ENVIRONMENT', 'CULTURE & ARTS']\n"
     ]
    }
   ],
   "source": [
    "news_data = pd.read_json(r\"News_Category_Dataset_v2.json\",lines=True)\n",
    "news_category = list(news_data[\"category\"].unique())\n",
    "print(news_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['World Cup', 'IPL', 'Scam', 'Environment', 'Rankings', 'Convention', 'Missing', 'Kamala Harris', 'Russia', 'Safety', 'Purdue', 'prohibit', 'Pandemic', 'Epidemic', 'Covid', 'CricketHockey', 'Basketball', 'Badminton', 'Football', 'Soccer', 'President', 'History', 'Games', 'Wildfire', 'Trump', 'Politics', 'Arrest', 'Shooting', 'Gathering', 'Threatning', 'Police', 'Vote', 'Election', 'Industry', 'Congress', 'Health', 'Entertainment', 'Destructive', 'Mystery', 'Summer', 'Risks', 'Fraud', 'Hospital', 'Community', 'Federal', 'Killer', 'Airline', 'Voting', 'Protest', 'Voilence', 'Lockdowns', 'President', 'Robbery', 'Education']\n"
     ]
    }
   ],
   "source": [
    "#some news items names manually added\n",
    "news_items = ['World Cup','IPL','Scam','Environment','Rankings','Convention','Missing','Kamala Harris','Russia','Safety',\n",
    "              'Purdue','prohibit','Pandemic','Epidemic','Covid','Cricket''Hockey','Basketball','Badminton','Football','Soccer',\n",
    "              'President','History','Games','Wildfire','Trump','Politics','Arrest','Shooting','Gathering','Threatning','Police',\n",
    "              'Vote','Election','Industry','Congress','Health','Entertainment','Destructive','Mystery','Summer','Risks','Fraud',\n",
    "              'Hospital','Community','Federal','Killer','Airline','Voting','Protest','Voilence','Lockdowns','President',\n",
    "              'Robbery','Education']\n",
    "print(news_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_terms_list = news_terms + news_category + news_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we have collected a list of query terms to fetch our data from the 'News API' as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the data using various news terms in newsapi \n",
    "def scraping_data(news_terms_list,apikey):\n",
    "    data = list()\n",
    "    for term in news_terms_list:\n",
    "        #if any term contains the spaces, it will replace it with '%20' value as url wont recognize for any empty spaces in between\n",
    "        term = term.replace(' ','%20')\n",
    "        url = \"http://newsapi.org/v2/everything?q=\"+str(term)+\"&apiKey=\"+str(apikey)\n",
    "        json_dumps = urlopen(url)\n",
    "        for article in json.loads(json_dumps.read())['articles']:\n",
    "            data.append(article)\n",
    "    print(str(len(data))+\" Documents extracted from various news sources\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4221 Documents extracted from various news sources\n"
     ]
    }
   ],
   "source": [
    "extracted_data = scraping_data(news_terms_list,apikey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, a total of 4221 Documents extracted from various news sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the scraped data:\n",
    "\n",
    "    In the JSON file collected above the source, the field has a name and id.\n",
    "\n",
    "    The id filed is sometimes having a null value, hence we are taking only the name field from the source.\n",
    "\n",
    "    To use the rank feature we are converting the 'publishedAt' value to timestamp and normalizing it.\n",
    "\n",
    "    The fields 'urlToImage' and 'content are being removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_data(data):\n",
    "    total_value = 0\n",
    "    #Modifiying source feature and removes the unwanted fields\n",
    "    for key,value in enumerate(data):\n",
    "        temp = value['source']['name']\n",
    "        data[key]['source'] = temp\n",
    "        date = datetime.datetime.strptime(value['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        timestamp = datetime.datetime.timestamp(date)\n",
    "        data[key]['timestamp'] = timestamp #creates a new field timestamp \n",
    "        \n",
    "        total_value += timestamp\n",
    "        del data[key]['urlToImage']\n",
    "        del data[key]['content']\n",
    "        \n",
    "    #Normalize the timestamp field\n",
    "    factor = 1.0/total_value\n",
    "    for key,value in enumerate(data):\n",
    "        data[key]['timestamp'] = value['timestamp'] *factor\n",
    "\n",
    "    print(\"Data cleaning/Formating has completed\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaning/Formating has completed\n"
     ]
    }
   ],
   "source": [
    "cleaned_data = cleaning_data(extracted_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Below, we are indexing the documents which are unique and have not been collected previously.\n",
    "\n",
    "* Based on this, the documents will be indexed from the last index previously created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing redundant data in any\n",
    "def redundancy_check(data, es, index_name):\n",
    "    new_data = list()\n",
    "    check_set = set()\n",
    "    #Finding number of documents present in our index\n",
    "    result = es.search(index=index_name)\n",
    "    doc_id = result['hits']['total']['value']\n",
    "    \n",
    "    #retreive all the data from the index and uses as a check_set load all titles present in index to avoid data repetition\n",
    "    res = es.search(index=index_name, body={\"from\" : 0, \"size\" : doc_id,\"query\": {\"match_all\" : {}}})\n",
    "    for each_doc in res['hits']['hits']:\n",
    "        check_set.add(each_doc['_source']['title'])\n",
    "    #removes the data redundancy not only w.r.to index data, but also looking into data prepared itself\n",
    "    for each_doc in data:\n",
    "        if each_doc[\"title\"] not in check_set:\n",
    "            new_data.append(each_doc)\n",
    "            check_set.add(each_doc[\"title\"])\n",
    "    print(\"There exists \"+str(len(new_data))+\" unique documents which are not present in our index and we can index this data\")\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There exists 3561 unique documents which are not present in our index and we can index this data\n"
     ]
    }
   ],
   "source": [
    "final_data = redundancy_check(cleaned_data, es,index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indexing_data function is used to index the data\n",
    "def indexing_data(data,es,index_name):\n",
    "    \n",
    "    #Checks the total documents present in the given index and stores this count in  doc_id variable\n",
    "    result = es.search(index=index_name)\n",
    "    doc_id = result['hits']['total']['value']\n",
    "    \n",
    "    #Loops over the data['articles'] dictionary and index each document into our index with sequential unique doc id\n",
    "    for i in data:\n",
    "        doc_id +=1\n",
    "        es.index(index=index_name, doc_type='_doc',id=doc_id, body=i)\n",
    "    print(\"Total \"+str(len(data))+\" documents indexed successfully into \"+index_name+\" index.\")\n",
    "    print(\"After indexing, Total \"+str(doc_id)+\" documents are present in this index\")\n",
    "    return doc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 3561 documents indexed successfully into ms4976_info624_201904_newsproject index.\n",
      "After indexing, Total 3561 documents are present in this index\n"
     ]
    }
   ],
   "source": [
    "doc_id = indexing_data(final_data,es, index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next we have the Custome Similarity Comparision Jupyter notebook which we have used for comparing different custom similarities for our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
